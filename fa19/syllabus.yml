- required:
    title: "Welcome to the Intelligence Group!"
    cover: "https://cdn-images-1.medium.com/max/1600/0*WbHkMBXJILlrXgYj.jpg"
    filename: "welcome"
    instructors: ["ionlights", "ahl98"]
    description: >-
      Welcome to the Intelligence group! This meeting, we'll be discerning
      everyone's research interests, whether passive or active. Following that,
      we'll start narrowing paper topics to fill the 5 unplanned meetings so
      everyone can begin getting a feel for the breadth of computation as a
      field of research.
  optional:
    tags: ["welcome", "deciding-on-research"]
    date: ""
    slides: ""
    papers: []
    kaggle:
      datasets: []
      competitions: []
      kernels: []

- required:
    title: "A Few Useful Things to Know about Machine Learning"
    cover: "https://cdn-images-1.medium.com/max/1600/0*WbHkMBXJILlrXgYj.jpg"
    filename: "useful-ml"
    instructors: ["ionlights", "ahl98"]
    description: >-
      Machine learning algorithms can figure out how to perform important tasks
      by generalizing from examples. This is often feasible and cost-effective
      where manual programming is not. As more data becomes available, more
      ambitious problems can be tackled. As a result, machine learning is widely
      used in computer science and other fields. However, developing successful
      machine learning applications requires a substantial amount of black art
      that is hard to find in textbooks. This article summarizes twelve key
      lessons that machine learning researchers and practitioners have learned.
      These include pitfalls to avoid, important issues to focus on, and answers
      to common questions.
  optional:
    tags: ["intro-paper"]
    date: ""
    slides: ""
    papers: []
    kaggle:
      datasets: []
      competitions: []
      kernels: []

- required:
    title: "Deep Learning"
    cover: "https://cdn-images-1.medium.com/max/1600/0*WbHkMBXJILlrXgYj.jpg"
    filename: "deep-learning"
    instructors: ["ionlights", "ahl98"]
    description: >-
      Abstract: Deep learning allows for computational models that are composed of
      multiple processing layers to learn representations of data with multiple
      levels of abstraction. These methods have dramatically improved the state-
      of-the-art in speech recognition, visual object recognition, object
      detection and many other domains such as drug discovery and genomics. Deep
      learning discovers intricate structure in large data sets by using the
      backpropagation algorithm to indicate how a machine should change its
      internal parameters that are used to compute the representation in each
      layer from the representation in the previous layer. Deep convolutional
      nets have brought about breakthroughs in processing images, video, speech
      and audio, whereas recurrent nets have shone light on sequential data such
      as text and speech.
  optional:
    tags: ["intro-paper", "review-paper", "deep-learning"]
    date: ""
    slides: ""
    papers: []
    kaggle:
      datasets: []
      competitions: []
      kernels: []

- required:
    title: "Speech Recognition with Deep Recurrent Neural Networks"
    cover: "https://cdn-images-1.medium.com/max/1600/0*WbHkMBXJILlrXgYj.jpg"
    filename: "deep-rnns"
    instructors: ["ionlights", "ahl98"]
    description: >-
      Our first non-review paper of the semester will be on using Deep RNNs to perform
      speech recognition tasks. This approach seeks to combine the advantages of deep
      neural networks wtih the "flexible use of long-range context that empowers RNNs".
      The abstract is rather lengthy, so I'll refrain from copying it here. Our weekly 
      meeting on this paper will go over questions from the paper, strategies for reading 
      more complex research papers, and how to identify strengths and weaknesses of journal 
      articles.

  optional:
    tags: ["journal-paper", "rnn", "speech-recognition", "deep-learning", "nlp"]
    date: ""
    slides: ""
    papers: ["https://arxiv.org/pdf/1303.5778.pdf"]
    kaggle:
      datasets: []
      competitions: []
      kernels: []

- required:
    title: "Attention Is All You Need"
    cover: "https://cdn-images-1.medium.com/max/1600/0*WbHkMBXJILlrXgYj.jpg"
    filename: "transformer"
    instructors: ["ionlights", "ahl98"]
    description: >-
      This paper, published from work performed at Google Brain and Google Research,
      proposes a new network architecture for tackling machine translation problems
      (among other ML transduction problems). This new approach simplifies the classic
      approach to translation while also achieving better performance. Accompanying the 
      paper is a Jupyter notebook created at Harvard to add annotations to the original 
      article while also supplying code mentioned in the work. This paper is most similar
      to the kinds of articles you can expect to be reading when doing original research.

  optional:
    tags: ["journal-paper", "nlp", "deep-learning"]
    date: ""
    slides: ""
    papers: ["https://arxiv.org/pdf/1706.03762.pdf"]
    kaggle:
      datasets: []
      competitions: []
      kernels: []

- required:
    title: "Placeholder (Paper decided on in 1st Meeting)"
    cover: "https://cdn-images-1.medium.com/max/1600/0*WbHkMBXJILlrXgYj.jpg"
    filename: "placeholder"
    instructors: ["ionlights", "ahl98"]
    description: >-

  optional:
    tags: ["intro-paper"]
    date: ""
    slides: ""
    papers: []
    kaggle:
      datasets: []
      competitions: []
      kernels: []

- required:
    title: "Placeholder (Paper decided on in 1st Meeting)"
    cover: "https://cdn-images-1.medium.com/max/1600/0*WbHkMBXJILlrXgYj.jpg"
    filename: "placeholder"
    instructors: ["ionlights", "ahl98"]
    description: >-

  optional:
    tags: ["intro-paper"]
    date: ""
    slides: ""
    papers: []
    kaggle:
      datasets: []
      competitions: []
      kernels: []

- required:
    title: "Placeholder (Paper decided on in 1st Meeting)"
    cover: "https://cdn-images-1.medium.com/max/1600/0*WbHkMBXJILlrXgYj.jpg"
    filename: "placeholder"
    instructors: ["ionlights", "ahl98"]
    description: >-

  optional:
    tags: ["intro-paper"]
    date: ""
    slides: ""
    papers: []
    kaggle:
      datasets: []
      competitions: []
      kernels: []

- required:
    title: "Building Machines That Learn and Think Like People"
    cover: "https://cdn-images-1.medium.com/max/1600/0*WbHkMBXJILlrXgYj.jpg"
    filename: "machines-learn-think-people"
    instructors: ["ionlights", "ahl98"]
    description: >-
      Recent progress in artificial intelligence (AI) has renewed interest in
      building systems that learn and think like people. Many advances have
      come from using deep neural networks trained end-to-end in tasks such as
      object recognition, video games, and board games, achieving performance
      that equals or even beats humans in some respects. Despite their
      biological inspiration and performance achievements, these systems differ
      from human intelligence in crucial ways. We review progress in cognitive
      science suggesting that truly human-like learning and thinking machines
      will have to reach beyond current engineering trends in both what they
      learn, and how they learn it. Specifically, we argue that these machines
      should (a) build causal models of the world that support explanation and
      understanding, rather than merely solving pattern recognition problems;
      (b) ground learning in intuitive theories of physics and psychology, to
      support and enrich the knowledge that is learned; and (c) harness
      compositionality and learning-to-learn to rapidly acquire and generalize
      knowledge to new tasks and situations. We suggest concrete challenges and
      promising routes towards these goals that can combine the strengths of
      recent neural network advances with more structured cognitive models.
  optional:
    tags: ["long-paper"]
    date: ""
    slides: ""
    papers: []
    kaggle:
      datasets: []
      competitions: []
      kernels: []


- required:
    title: "Building Machines That Learn and Think For Themselves"
    cover: "https://cdn-images-1.medium.com/max/1600/0*WbHkMBXJILlrXgYj.jpg"
    filename: "machines-learn-think-themselves"
    instructors: ["ionlights", "ahl98"]
    description: >-
      We agree with Lake and colleagues on their list of key ingredients for
      building humanlike intelligence, including the idea that model-based
      reasoning is essential. However, we favor an approach that centers on one
      additional ingredient: autonomy. In particular, we aim toward agents that
      can both build and exploit their own internal models, with minimal human
      hand-engineering. We believe an approach centered on autonomous learning
      has the greatest chance of success as we scale toward real-world
      complexity, tackling domains for which ready-made formal models are not
      available. Here we survey several important examples of the progress that
      has been made toward building autonomous agents with humanlike abilities,
      and highlight some outstanding challenges.
  optional:
    tags: ["long-paper"]
    date: ""
    slides: ""
    papers: []
    kaggle:
      datasets: []
      competitions: []
      kernels: []

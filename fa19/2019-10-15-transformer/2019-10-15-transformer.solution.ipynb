{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nb-title": true,
    "title": "Attention Is All You Need"
   },
   "source": [
    "<img src=\"https://ucfai.org/intelligence/fa19/2019-10-15-transformer/transformer/banner.png\">\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <span class=\"btn btn-success btn-block\">\n",
    "        Meeting in-person? Have you signed in?\n",
    "    </span>\n",
    "</div>\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <h1> Attention Is All You Need </h1>\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "<div style=\"line-height: 2em;\">\n",
    "    <p>by: \n",
    "        <strong> John Muchovej</strong>\n",
    "        (<a href=\"https://github.com/ionlights\">@ionlights</a>)\n",
    "    \n",
    "        <strong> None</strong>\n",
    "        (<a href=\"https://github.com/ahl98\">@ahl98</a>)\n",
    "     on 2019-10-15</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "autobot": {
   "authors": [
    {
     "author": "John Muchovej",
     "github": "ionlights",
     "web": null
    },
    {
     "author": null,
     "github": "ahl98",
     "web": null
    }
   ],
   "categories": [
    "fa19"
   ],
   "date": "2019-10-15",
   "description": "This paper, published from work performed at Google Brain and Google Research, proposes a new network architecture for tackling machine translation problems (among other ML transduction problems). This new approach simplifies the classic approach to translation while also achieving better performance. Accompanying the  paper is a Jupyter notebook created at Harvard to add annotations to the original  article while also supplying code mentioned in the work. This paper is most similar to the kinds of articles you can expect to be reading when doing original research.",
   "tags": [
    "journal-paper",
    "nlp",
    "deep-learning"
   ],
   "title": "Attention Is All You Need"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
